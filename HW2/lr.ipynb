{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Линейная регрессия\n",
    "\n",
    "## Цели работы:\n",
    "1. реализовать два способа решения задачи линейной регрессии;\n",
    "2. настроить гиперпараметры у каждого алгоритма, в частности параметры одного из методов регуляризации;\n",
    "3. анализ результатов.\n",
    "\n",
    "## Данные\n",
    "\n",
    "Используйте один из этих наборов данных для тестирования алгоритмов. Каждый тест в архиве организован следующим образом:\n",
    "\n",
    "%число признаков%\n",
    "\n",
    "%число объектов в тренировочном наборе%\n",
    "\n",
    "%объект тренировочного набора 1%\n",
    "\n",
    "%объект тренировочного набора 2%\n",
    "\n",
    "…..\n",
    "\n",
    "%объект тренировочного набора N%\n",
    "\n",
    "%число объектов в тестовом наборе%\n",
    "\n",
    "%объект тестового набора 1%\n",
    "\n",
    "%объект тестового набора 2%\n",
    "\n",
    "…..\n",
    "\n",
    "%объект тестового набора K%\n",
    "\n",
    "Формат объектов совпадает с форматом из соответствующей задачи на Codeforces.\n",
    "\n",
    "## Задание\n",
    "\n",
    "### Алгоритмы\n",
    "\n",
    "Реализуйте алгоритмы нахождения уравнения прямой для задачи линейной регрессии:\n",
    "- МНК — метод наименьших квадратов (псевдообратная матрица / SVD);\n",
    "- градиентный спуск.\n",
    "\n",
    "На лекции мы рассматривали алгоритм градиентного спуска для классификации, однако его можно применять и для задач регрессии, важно лишь выбрать дифференцируемую функцию ошибки. В данном случае необходимо использовать среднюю квадратичную ошибку.\n",
    "\n",
    "Требуется реализовать стохастический или пакетный градиентный спуск. Напоминаем, что эмпирический риск нужно балансировать на каждой итерации при помощи экспоненциального скользящего среднего.\n",
    "\n",
    "Для алгоритма градиентного спуска рекомендуется использовать начальную инициализацию весов где — число признаков (см. лекцию). Шаг градиента необходимо уменьшать на каждой итерации, например: — номер итерации. Другие способы инициализации весов и уменьшения шага градиента использовать также можно.\n",
    "\n",
    "Алгоритм градиентного спуска необходимо запустить с ограничением по числу итераций (не более 2000 итераций).\n",
    "\n",
    "В качестве функции оценки качества алгоритма используйте NRMSE, либо SMAPE.\n",
    "\n",
    "### Регуляризация\n",
    "\n",
    "В реализации каждого из вышеупомянутых алгоритмов необходимо использовать регуляризацию. Для МНК гребневую регуляризацию, для градиентного один из методов на выбор:\n",
    "- гребневая;\n",
    "- LASSO;\n",
    "- Elastic Net.\n",
    "\n",
    "### Настройка и анализ\n",
    "\n",
    "Для каждого алгоритма найдите наилучшие гиперпараметры, а именно, параметры регуляризации, и выведите лучшие соответствующие результаты с точки зрения выбранной Вами функции ошибки.  Перебирать различные способы инициализации вектора весов, уменьшения шага градиента а также различные темпы затухания в экспоненциальном скользящем среднем в качестве гиперпараметров не требуется.\n",
    "\n",
    "Для алгоритма градиентного спуска постройте график зависимости функции оценки качества (NRMSE или SMAPE) на тренировочном и тестовом множестве от числа итераций.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import linalg\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_data():\n",
    "    data = []\n",
    "    for i in range(1,8):\n",
    "        with open(f\"LR/{i}.txt\") as f:\n",
    "            lines=f.readlines()\n",
    "            n_objects_line = 1\n",
    "            for type in ['train', 'test']:\n",
    "                try:\n",
    "                    objects_size = int(lines[n_objects_line])\n",
    "                    start_object = n_objects_line + 1\n",
    "                    end_object = n_objects_line + objects_size + 1\n",
    "                    \n",
    "                    data.append({\"file_name\": i,\n",
    "                                \"type\": type,\n",
    "                                \"n_features\": int(lines[0].splitlines()[0]), \n",
    "                                \"n_objects\": int(lines[n_objects_line].splitlines()[0]), \n",
    "                                \"objects\": pd.DataFrame(line.splitlines()[0].split(' ') for line in lines[start_object: end_object]).applymap(int)\n",
    "                                })\n",
    "                    n_objects_line = end_object\n",
    "                except:\n",
    "                    break\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def normalize(data):\n",
    "    return data/data.max()\n",
    "\n",
    "def prepare_data():\n",
    "    data = get_data()[6:8]\n",
    "\n",
    "    data_train = data[0][\"objects\"]\n",
    "    data_test = data[1][\"objects\"]\n",
    "\n",
    "    y_train = data_train.iloc[:,-1]\n",
    "    X_train = data_train.iloc[:,:-1]\n",
    "\n",
    "    y_test = data_test.iloc[:,-1]\n",
    "    X_test = data_test.iloc[:,:-1]\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_ = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test_ = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "    return X_train_, y_train, X_test_, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "X_train, y_train, X_test, y_test = prepare_data()\n",
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0         1         2    3         4         5         6    \\\n",
       "0     0.290880  0.523379  0.365296  0.0  0.741999  0.712896  0.428240   \n",
       "1     0.506202  0.183183  0.623553  0.0  0.435299  0.907144  0.888649   \n",
       "2     0.761521  0.289316  0.696488  0.0  0.021457  0.575692  0.683432   \n",
       "3     0.210820  0.438913  0.330093  0.0  0.520178  0.228544  0.093204   \n",
       "4     0.138383  0.204623  0.553441  0.0  0.167169  0.046879  0.654724   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3410  0.695474  0.485555  0.450929  0.0  0.776288  0.586007  0.933825   \n",
       "3411  0.868773  0.776879  0.259971  0.0  0.136245  0.664516  0.147086   \n",
       "3412  0.318023  0.614894  0.451972  0.0  0.125179  0.189062  0.114757   \n",
       "3413  0.253518  0.471645  0.487435  0.0  0.592317  0.708410  0.386124   \n",
       "3414  0.390951  0.512094  0.524447  0.0  0.220316  0.014645  0.100721   \n",
       "\n",
       "           7         8         9    ...       136       137       138  \\\n",
       "0     0.398463  0.663590  0.962739  ...  0.448601  0.523379  0.590153   \n",
       "1     0.391958  0.436566  0.617285  ...  0.420687  0.183183  0.373206   \n",
       "2     0.660527  0.489378  0.410974  ...  0.720175  0.289316  0.307313   \n",
       "3     0.478943  0.744814  0.588096  ...  0.396561  0.438913  0.652635   \n",
       "4     0.303053  0.812492  0.988668  ...  0.176559  0.204623  0.422688   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3410  0.401546  0.324801  0.579772  ...  0.530461  0.485555  0.564645   \n",
       "3411  0.424015  0.300813  0.340503  ...  0.630304  0.776879  0.680013   \n",
       "3412  0.487322  0.362809  0.314545  ...  0.387402  0.614894  0.560295   \n",
       "3413  0.262379  0.647789  0.280820  ...  0.331559  0.471645  0.471538   \n",
       "3414  0.252299  0.162248  0.653208  ...  0.290389  0.512094  0.452137   \n",
       "\n",
       "           139       140       141       142       143       144       145  \n",
       "0     0.451278  0.841315  0.382914  0.212410  0.373709  0.613884  0.173201  \n",
       "1     0.715474  0.652210  0.628019  0.529766  0.637487  0.415861  0.832451  \n",
       "2     0.056902  0.806782  0.672165  0.550110  0.698601  0.210246  0.639962  \n",
       "3     0.233141  0.909026  0.384705  0.392110  0.341457  0.566577  0.044235  \n",
       "4     0.815524  0.782208  0.529701  0.655195  0.568925  0.571580  0.309334  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3410  0.643305  0.435305  0.504701  0.537562  0.454950  0.252946  0.845815  \n",
       "3411  0.447234  0.678714  0.487064  0.657695  0.253485  0.381803  0.854296  \n",
       "3412  0.178909  0.101956  0.287401  0.553631  0.450617  0.375163  0.378828  \n",
       "3413  0.737613  0.901193  0.526945  0.647963  0.490358  0.410291  0.108838  \n",
       "3414  0.568294  0.065368  0.497467  0.551980  0.521865  0.389912  0.091409  \n",
       "\n",
       "[3415 rows x 146 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.290880</td>\n",
       "      <td>0.523379</td>\n",
       "      <td>0.365296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741999</td>\n",
       "      <td>0.712896</td>\n",
       "      <td>0.428240</td>\n",
       "      <td>0.398463</td>\n",
       "      <td>0.663590</td>\n",
       "      <td>0.962739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448601</td>\n",
       "      <td>0.523379</td>\n",
       "      <td>0.590153</td>\n",
       "      <td>0.451278</td>\n",
       "      <td>0.841315</td>\n",
       "      <td>0.382914</td>\n",
       "      <td>0.212410</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.613884</td>\n",
       "      <td>0.173201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506202</td>\n",
       "      <td>0.183183</td>\n",
       "      <td>0.623553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435299</td>\n",
       "      <td>0.907144</td>\n",
       "      <td>0.888649</td>\n",
       "      <td>0.391958</td>\n",
       "      <td>0.436566</td>\n",
       "      <td>0.617285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420687</td>\n",
       "      <td>0.183183</td>\n",
       "      <td>0.373206</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>0.652210</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.529766</td>\n",
       "      <td>0.637487</td>\n",
       "      <td>0.415861</td>\n",
       "      <td>0.832451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.761521</td>\n",
       "      <td>0.289316</td>\n",
       "      <td>0.696488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.575692</td>\n",
       "      <td>0.683432</td>\n",
       "      <td>0.660527</td>\n",
       "      <td>0.489378</td>\n",
       "      <td>0.410974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720175</td>\n",
       "      <td>0.289316</td>\n",
       "      <td>0.307313</td>\n",
       "      <td>0.056902</td>\n",
       "      <td>0.806782</td>\n",
       "      <td>0.672165</td>\n",
       "      <td>0.550110</td>\n",
       "      <td>0.698601</td>\n",
       "      <td>0.210246</td>\n",
       "      <td>0.639962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210820</td>\n",
       "      <td>0.438913</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520178</td>\n",
       "      <td>0.228544</td>\n",
       "      <td>0.093204</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>0.744814</td>\n",
       "      <td>0.588096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396561</td>\n",
       "      <td>0.438913</td>\n",
       "      <td>0.652635</td>\n",
       "      <td>0.233141</td>\n",
       "      <td>0.909026</td>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.392110</td>\n",
       "      <td>0.341457</td>\n",
       "      <td>0.566577</td>\n",
       "      <td>0.044235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138383</td>\n",
       "      <td>0.204623</td>\n",
       "      <td>0.553441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167169</td>\n",
       "      <td>0.046879</td>\n",
       "      <td>0.654724</td>\n",
       "      <td>0.303053</td>\n",
       "      <td>0.812492</td>\n",
       "      <td>0.988668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176559</td>\n",
       "      <td>0.204623</td>\n",
       "      <td>0.422688</td>\n",
       "      <td>0.815524</td>\n",
       "      <td>0.782208</td>\n",
       "      <td>0.529701</td>\n",
       "      <td>0.655195</td>\n",
       "      <td>0.568925</td>\n",
       "      <td>0.571580</td>\n",
       "      <td>0.309334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>0.695474</td>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.450929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776288</td>\n",
       "      <td>0.586007</td>\n",
       "      <td>0.933825</td>\n",
       "      <td>0.401546</td>\n",
       "      <td>0.324801</td>\n",
       "      <td>0.579772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530461</td>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.564645</td>\n",
       "      <td>0.643305</td>\n",
       "      <td>0.435305</td>\n",
       "      <td>0.504701</td>\n",
       "      <td>0.537562</td>\n",
       "      <td>0.454950</td>\n",
       "      <td>0.252946</td>\n",
       "      <td>0.845815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>0.868773</td>\n",
       "      <td>0.776879</td>\n",
       "      <td>0.259971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136245</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.147086</td>\n",
       "      <td>0.424015</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.340503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630304</td>\n",
       "      <td>0.776879</td>\n",
       "      <td>0.680013</td>\n",
       "      <td>0.447234</td>\n",
       "      <td>0.678714</td>\n",
       "      <td>0.487064</td>\n",
       "      <td>0.657695</td>\n",
       "      <td>0.253485</td>\n",
       "      <td>0.381803</td>\n",
       "      <td>0.854296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>0.318023</td>\n",
       "      <td>0.614894</td>\n",
       "      <td>0.451972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125179</td>\n",
       "      <td>0.189062</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>0.487322</td>\n",
       "      <td>0.362809</td>\n",
       "      <td>0.314545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387402</td>\n",
       "      <td>0.614894</td>\n",
       "      <td>0.560295</td>\n",
       "      <td>0.178909</td>\n",
       "      <td>0.101956</td>\n",
       "      <td>0.287401</td>\n",
       "      <td>0.553631</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>0.375163</td>\n",
       "      <td>0.378828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>0.253518</td>\n",
       "      <td>0.471645</td>\n",
       "      <td>0.487435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592317</td>\n",
       "      <td>0.708410</td>\n",
       "      <td>0.386124</td>\n",
       "      <td>0.262379</td>\n",
       "      <td>0.647789</td>\n",
       "      <td>0.280820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331559</td>\n",
       "      <td>0.471645</td>\n",
       "      <td>0.471538</td>\n",
       "      <td>0.737613</td>\n",
       "      <td>0.901193</td>\n",
       "      <td>0.526945</td>\n",
       "      <td>0.647963</td>\n",
       "      <td>0.490358</td>\n",
       "      <td>0.410291</td>\n",
       "      <td>0.108838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>0.390951</td>\n",
       "      <td>0.512094</td>\n",
       "      <td>0.524447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220316</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.100721</td>\n",
       "      <td>0.252299</td>\n",
       "      <td>0.162248</td>\n",
       "      <td>0.653208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290389</td>\n",
       "      <td>0.512094</td>\n",
       "      <td>0.452137</td>\n",
       "      <td>0.568294</td>\n",
       "      <td>0.065368</td>\n",
       "      <td>0.497467</td>\n",
       "      <td>0.551980</td>\n",
       "      <td>0.521865</td>\n",
       "      <td>0.389912</td>\n",
       "      <td>0.091409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3415 rows × 146 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVD"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "X_train, y_train, X_test, y_test = prepare_data()\n",
    "\n",
    "U, S, Vh = np.linalg.svd(X_train)\n",
    "svd =  TruncatedSVD(n_components = 2)\n",
    "A_transformed = svd.fit_transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "# check\n",
    "S_ = linalg.diagsvd(S, X_train.shape[0], X_train.shape[1])\n",
    "train_transformed = np.dot(U, np.dot(S_, Vh))\n",
    "np.any(np.absolute(X_train - train_transformed) > 0.01)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "clf = Ridge(solver='svd')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "clf.get_params()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'normalize': 'deprecated',\n",
       " 'positive': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'svd',\n",
       " 'tol': 0.001}"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "r2_score(y_test, y_pred)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9796698905020927"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "scores = dict()\n",
    "for alpha in np.arange(1, 11, 0.1):\n",
    "    clf = Ridge(solver='svd') #sag\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    scores[round(alpha,1)] = r2_score(y_test, y_pred)\n",
    "best_alpha = max(scores.items(), key=lambda t: t[1])\n",
    "best_alpha\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.0, 0.9796698905020927)"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "for k,v in scores.items():\n",
    "    print(f\"alpha={k}, r2_score={v}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha=1.0, r2_score=0.9796698905020927\n",
      "alpha=1.1, r2_score=0.9796698905020927\n",
      "alpha=1.2, r2_score=0.9796698905020927\n",
      "alpha=1.3, r2_score=0.9796698905020927\n",
      "alpha=1.4, r2_score=0.9796698905020927\n",
      "alpha=1.5, r2_score=0.9796698905020927\n",
      "alpha=1.6, r2_score=0.9796698905020927\n",
      "alpha=1.7, r2_score=0.9796698905020927\n",
      "alpha=1.8, r2_score=0.9796698905020927\n",
      "alpha=1.9, r2_score=0.9796698905020927\n",
      "alpha=2.0, r2_score=0.9796698905020927\n",
      "alpha=2.1, r2_score=0.9796698905020927\n",
      "alpha=2.2, r2_score=0.9796698905020927\n",
      "alpha=2.3, r2_score=0.9796698905020927\n",
      "alpha=2.4, r2_score=0.9796698905020927\n",
      "alpha=2.5, r2_score=0.9796698905020927\n",
      "alpha=2.6, r2_score=0.9796698905020927\n",
      "alpha=2.7, r2_score=0.9796698905020927\n",
      "alpha=2.8, r2_score=0.9796698905020927\n",
      "alpha=2.9, r2_score=0.9796698905020927\n",
      "alpha=3.0, r2_score=0.9796698905020927\n",
      "alpha=3.1, r2_score=0.9796698905020927\n",
      "alpha=3.2, r2_score=0.9796698905020927\n",
      "alpha=3.3, r2_score=0.9796698905020927\n",
      "alpha=3.4, r2_score=0.9796698905020927\n",
      "alpha=3.5, r2_score=0.9796698905020927\n",
      "alpha=3.6, r2_score=0.9796698905020927\n",
      "alpha=3.7, r2_score=0.9796698905020927\n",
      "alpha=3.8, r2_score=0.9796698905020927\n",
      "alpha=3.9, r2_score=0.9796698905020927\n",
      "alpha=4.0, r2_score=0.9796698905020927\n",
      "alpha=4.1, r2_score=0.9796698905020927\n",
      "alpha=4.2, r2_score=0.9796698905020927\n",
      "alpha=4.3, r2_score=0.9796698905020927\n",
      "alpha=4.4, r2_score=0.9796698905020927\n",
      "alpha=4.5, r2_score=0.9796698905020927\n",
      "alpha=4.6, r2_score=0.9796698905020927\n",
      "alpha=4.7, r2_score=0.9796698905020927\n",
      "alpha=4.8, r2_score=0.9796698905020927\n",
      "alpha=4.9, r2_score=0.9796698905020927\n",
      "alpha=5.0, r2_score=0.9796698905020927\n",
      "alpha=5.1, r2_score=0.9796698905020927\n",
      "alpha=5.2, r2_score=0.9796698905020927\n",
      "alpha=5.3, r2_score=0.9796698905020927\n",
      "alpha=5.4, r2_score=0.9796698905020927\n",
      "alpha=5.5, r2_score=0.9796698905020927\n",
      "alpha=5.6, r2_score=0.9796698905020927\n",
      "alpha=5.7, r2_score=0.9796698905020927\n",
      "alpha=5.8, r2_score=0.9796698905020927\n",
      "alpha=5.9, r2_score=0.9796698905020927\n",
      "alpha=6.0, r2_score=0.9796698905020927\n",
      "alpha=6.1, r2_score=0.9796698905020927\n",
      "alpha=6.2, r2_score=0.9796698905020927\n",
      "alpha=6.3, r2_score=0.9796698905020927\n",
      "alpha=6.4, r2_score=0.9796698905020927\n",
      "alpha=6.5, r2_score=0.9796698905020927\n",
      "alpha=6.6, r2_score=0.9796698905020927\n",
      "alpha=6.7, r2_score=0.9796698905020927\n",
      "alpha=6.8, r2_score=0.9796698905020927\n",
      "alpha=6.9, r2_score=0.9796698905020927\n",
      "alpha=7.0, r2_score=0.9796698905020927\n",
      "alpha=7.1, r2_score=0.9796698905020927\n",
      "alpha=7.2, r2_score=0.9796698905020927\n",
      "alpha=7.3, r2_score=0.9796698905020927\n",
      "alpha=7.4, r2_score=0.9796698905020927\n",
      "alpha=7.5, r2_score=0.9796698905020927\n",
      "alpha=7.6, r2_score=0.9796698905020927\n",
      "alpha=7.7, r2_score=0.9796698905020927\n",
      "alpha=7.8, r2_score=0.9796698905020927\n",
      "alpha=7.9, r2_score=0.9796698905020927\n",
      "alpha=8.0, r2_score=0.9796698905020927\n",
      "alpha=8.1, r2_score=0.9796698905020927\n",
      "alpha=8.2, r2_score=0.9796698905020927\n",
      "alpha=8.3, r2_score=0.9796698905020927\n",
      "alpha=8.4, r2_score=0.9796698905020927\n",
      "alpha=8.5, r2_score=0.9796698905020927\n",
      "alpha=8.6, r2_score=0.9796698905020927\n",
      "alpha=8.7, r2_score=0.9796698905020927\n",
      "alpha=8.8, r2_score=0.9796698905020927\n",
      "alpha=8.9, r2_score=0.9796698905020927\n",
      "alpha=9.0, r2_score=0.9796698905020927\n",
      "alpha=9.1, r2_score=0.9796698905020927\n",
      "alpha=9.2, r2_score=0.9796698905020927\n",
      "alpha=9.3, r2_score=0.9796698905020927\n",
      "alpha=9.4, r2_score=0.9796698905020927\n",
      "alpha=9.5, r2_score=0.9796698905020927\n",
      "alpha=9.6, r2_score=0.9796698905020927\n",
      "alpha=9.7, r2_score=0.9796698905020927\n",
      "alpha=9.8, r2_score=0.9796698905020927\n",
      "alpha=9.9, r2_score=0.9796698905020927\n",
      "alpha=10.0, r2_score=0.9796698905020927\n",
      "alpha=10.1, r2_score=0.9796698905020927\n",
      "alpha=10.2, r2_score=0.9796698905020927\n",
      "alpha=10.3, r2_score=0.9796698905020927\n",
      "alpha=10.4, r2_score=0.9796698905020927\n",
      "alpha=10.5, r2_score=0.9796698905020927\n",
      "alpha=10.6, r2_score=0.9796698905020927\n",
      "alpha=10.7, r2_score=0.9796698905020927\n",
      "alpha=10.8, r2_score=0.9796698905020927\n",
      "alpha=10.9, r2_score=0.9796698905020927\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(actual - predicted))\n",
    "\n",
    "\n",
    "def rmse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Root Mean Squared Error \"\"\"\n",
    "    return np.sqrt(mse(actual, predicted))\n",
    "\n",
    "def NRMSE(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Normalized Root Mean Squared Error \"\"\"\n",
    "    return rmse(actual, predicted) / (actual.max() - actual.min())\n",
    "\n",
    "def smape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return np.mean(2.0 * np.abs(actual - predicted) / ((np.abs(actual) + np.abs(predicted)) + EPSILON))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alpha = list(np.arange(0.1, 1, 0.01))\n",
    "penalty = ['l2', 'l1', 'elasticnet']\n",
    "hyperparameters = dict(alpha=alpha, penalty = penalty)\n",
    "\n",
    "regr = SGDRegressor(max_iter=1000)\n",
    "clf = GridSearchCV(regr, hyperparameters, cv=20)\n",
    "model = clf.fit(X_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "model.best_estimator_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.7399999999999997, penalty='l1')"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "scores_smape = dict()\n",
    "\n",
    "for iteration in range(1, 2000, 10):\n",
    "    regr = SGDRegressor(max_iter=iteration, penalty='l1', alpha=0.74)\n",
    "    regr.fit(X_train, y_train)\n",
    "    scores_smape[iteration] = smape(y_test, regr.predict(X_test))\n",
    "\n",
    "sns.lineplot(x=scores_smape.keys(), y=scores_smape.values())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "41dad33ac27be026cb602e0a62f760d014741d4939993a4186ae922aa189125a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}